# 梯度累积 vs 数据并行：同一原理的不同形式
## 数学等价性深度剖析

---

## 目录
1. [核心洞察](#1-核心洞察)
2. [梯度累积原理](#2-梯度累积原理)
3. [数据并行原理](#3-数据并行原理)
4. [数学等价性证明](#4-数学等价性证明)
5. [时间和空间权衡](#5-时间和空间权衡)
6. [混合使用](#6-混合使用)
7. [实际应用场景](#7-实际应用场景)

---

## 1. 核心洞察

### 1.1 你的直觉完全正确！

```
┌─────────────────────────────────────────────────────────┐
│         梯度累积 和 数据并行 的本质是相同的             │
├─────────────────────────────────────────────────────────┤
│                                                          │
│ 核心数学原理: 梯度的可加性                              │
│                                                          │
│ ∂(L₁+L₂+...+Lₙ)/∂w = ∂L₁/∂w + ∂L₂/∂w + ... + ∂Lₙ/∂w │
│                                                          │
│ 两者都是:                                                │
│ 1. 将大 batch 分成小块                                  │
│ 2. 分别计算每块的梯度                                   │
│ 3. 累加所有梯度                                         │
│ 4. 求平均后更新参数                                     │
│                                                          │
│ 区别只在于:                                             │
│ - 梯度累积: 在时间上分开 (串行)                        │
│ - 数据并行: 在空间上分开 (并行)                        │
│                                                          │
└─────────────────────────────────────────────────────────┘
```

### 1.2 直观对比

```
目标: 处理 128 张图片

方法 1: 直接训练 (基准)
═══════════════════════════════════════════════════════════
一次性处理 128 张
显存需求: 很高
时间: T

方法 2: 梯度累积 (时间维度拆分)
═══════════════════════════════════════════════════════════
分 4 步，每步 32 张:
  Step 1: 处理 img 0-31   → 计算 grad₁
  Step 2: 处理 img 32-63  → 计算 grad₂, 累加 grad₁
  Step 3: 处理 img 64-95  → 计算 grad₃, 累加 grad₁+grad₂
  Step 4: 处理 img 96-127 → 计算 grad₄, 累加所有
  更新参数: w -= lr * (grad₁+grad₂+grad₃+grad₄) / 4

显存需求: 低 (只需处理 32 张)
时间: 4T (串行)

方法 3: 数据并行 (空间维度拆分)
═══════════════════════════════════════════════════════════
同时在 4 个 GPU:
  GPU 0: 处理 img 0-31   → 计算 grad₁
  GPU 1: 处理 img 32-63  → 计算 grad₂
  GPU 2: 处理 img 64-95  → 计算 grad₃
  GPU 3: 处理 img 96-127 → 计算 grad₄
  All-Reduce: grad_avg = (grad₁+grad₂+grad₃+grad₄) / 4
  更新参数: w -= lr * grad_avg

显存需求: 低 (每 GPU 处理 32 张)
时间: T (并行)

数学上: 方法 1 = 方法 2 = 方法 3
效率上: 方法 3 最快，方法 2 省显存
```

---

## 2. 梯度累积原理

### 2.1 什么是梯度累积？

```python
# 传统训练 (不用梯度累积)
# ══════════════════════════════════════════════════════

for batch in dataloader:  # batch_size = 128
    optimizer.zero_grad()
    
    # 前向
    outputs = model(batch)  # 一次处理 128 张
    loss = criterion(outputs, labels)
    
    # 反向
    loss.backward()  # 计算梯度
    
    # 更新
    optimizer.step()  # 立即更新参数
```

```python
# 梯度累积训练
# ══════════════════════════════════════════════════════

accumulation_steps = 4  # 累积 4 步
effective_batch_size = 32 * 4 = 128

for batch in dataloader:  # batch_size = 32 (小!)
    
    # 前向
    outputs = model(batch)  # 一次只处理 32 张
    loss = criterion(outputs, labels) / accumulation_steps  # 除以步数!
    
    # 反向 (不清零梯度，累加!)
    loss.backward()  # grad += ∂loss/∂w
    
    # 每 4 步更新一次
    if (step + 1) % accumulation_steps == 0:
        optimizer.step()       # 更新参数
        optimizer.zero_grad()  # 清零梯度
```

### 2.2 梯度累积的数学过程

```python
# ========== 详细展开 ==========

# Step 1: 处理 mini-batch 1 (img 0-31)
loss₁ = mean(L₀, L₁, ..., L₃₁) / 4
loss₁.backward()
# 此时: grad = ∂loss₁/∂w = (∂L₀ + ... + ∂L₃₁) / (32 × 4)

# Step 2: 处理 mini-batch 2 (img 32-63)
loss₂ = mean(L₃₂, L₃₃, ..., L₆₃) / 4
loss₂.backward()
# 此时: grad += ∂loss₂/∂w
#      grad = (∂L₀ + ... + ∂L₃₁) / (32×4) + (∂L₃₂ + ... + ∂L₆₃) / (32×4)
#           = (∂L₀ + ... + ∂L₆₃) / (32 × 4)

# Step 3: 处理 mini-batch 3 (img 64-95)
loss₃ = mean(L₆₄, L₆₅, ..., L₉₅) / 4
loss₃.backward()
# grad = (∂L₀ + ... + ∂L₉₅) / (32 × 4)

# Step 4: 处理 mini-batch 4 (img 96-127)
loss₄ = mean(L₉₆, L₉₇, ..., L₁₂₇) / 4
loss₄.backward()
# grad = (∂L₀ + ... + ∂L₁₂₇) / (32 × 4)
#      = (∂L₀ + ... + ∂L₁₂₇) / 128

# Step 5: 更新参数
optimizer.step()  # w -= lr × grad
```

### 2.3 为什么要除以 accumulation_steps？

```python
# 关键理解: loss 的归一化

# 错误做法 (不除以 accumulation_steps):
# ────────────────────────────────────────────────────
for step in range(4):
    loss = criterion(outputs, labels)  # 不除!
    loss.backward()

# 结果:
# grad = ∂loss₁/∂w + ∂loss₂/∂w + ∂loss₃/∂w + ∂loss₄/∂w
#      = (∂L₀+...+∂L₃₁)/32 + ... + (∂L₉₆+...+∂L₁₂₇)/32
#      = (∂L₀+...+∂L₁₂₇) / 32  ← 错了! 应该是 /128

# 正确做法 (除以 accumulation_steps = 4):
# ────────────────────────────────────────────────────
for step in range(4):
    loss = criterion(outputs, labels) / 4  # 除以 4!
    loss.backward()

# 结果:
# grad = ∂(loss₁/4)/∂w + ... + ∂(loss₄/4)/∂w
#      = [(∂L₀+...+∂L₃₁)/32]/4 + ... + [(∂L₉₆+...+∂L₁₂₇)/32]/4
#      = (∂L₀+...+∂L₁₂₇) / 128  ← 正确!

# 或者等价写法:
for step in range(4):
    loss = criterion(outputs, labels)
    (loss / 4).backward()  # 效果一样
```

---

## 3. 数据并行原理 (回顾)

### 3.1 数据并行的数学过程

```python
# 4 个 GPU 同时工作

# GPU 0: 处理 img 0-31
loss₀ = mean(L₀, L₁, ..., L₃₁)
loss₀.backward()
grad₀ = (∂L₀ + ... + ∂L₃₁) / 32

# GPU 1: 处理 img 32-63
loss₁ = mean(L₃₂, L₃₃, ..., L₆₃)
loss₁.backward()
grad₁ = (∂L₃₂ + ... + ∂L₆₃) / 32

# GPU 2: 处理 img 64-95
grad₂ = (∂L₆₄ + ... + ∂L₉₅) / 32

# GPU 3: 处理 img 96-127
grad₃ = (∂L₉₆ + ... + ∂L₁₂₇) / 32

# All-Reduce 求平均
grad_avg = (grad₀ + grad₁ + grad₂ + grad₃) / 4
         = [(∂L₀+...+∂L₃₁)/32 + ... + (∂L₉₆+...+∂L₁₂₇)/32] / 4
         = (∂L₀ + ... + ∂L₁₂₇) / 128

# 更新参数
w -= lr × grad_avg
```

---

## 4. 数学等价性证明

### 4.1 三种方法的公式对比

```
方法 1: 直接训练 (batch_size=128)
═══════════════════════════════════════════════════════════
loss = (L₀ + L₁ + ... + L₁₂₇) / 128
grad = ∂loss/∂w = (∂L₀ + ... + ∂L₁₂₇) / 128

更新: w -= lr × grad


方法 2: 梯度累积 (4 步，每步 32)
═══════════════════════════════════════════════════════════
Step 1: loss₁ = (L₀ + ... + L₃₁) / 32 / 4
        grad₁ = (∂L₀ + ... + ∂L₃₁) / 128

Step 2: loss₂ = (L₃₂ + ... + L₆₃) / 32 / 4
        grad₂ = (∂L₃₂ + ... + ∂L₆₃) / 128

Step 3: loss₃ = (L₆₄ + ... + L₉₅) / 32 / 4
        grad₃ = (∂L₆₄ + ... + ∂L₉₅) / 128

Step 4: loss₄ = (L₉₆ + ... + L₁₂₇) / 32 / 4
        grad₄ = (∂L₉₆ + ... + ∂L₁₂₇) / 128

累积梯度: grad = grad₁ + grad₂ + grad₃ + grad₄
              = (∂L₀ + ... + ∂L₁₂₇) / 128

更新: w -= lr × grad


方法 3: 数据并行 (4 GPU，各 32 张)
═══════════════════════════════════════════════════════════
GPU 0: grad₀ = (∂L₀ + ... + ∂L₃₁) / 32
GPU 1: grad₁ = (∂L₃₂ + ... + ∂L₆₃) / 32
GPU 2: grad₂ = (∂L₆₄ + ... + ∂L₉₅) / 32
GPU 3: grad₃ = (∂L₉₆ + ... + ∂L₁₂₇) / 32

All-Reduce: grad = (grad₀ + grad₁ + grad₂ + grad₃) / 4
                 = (∂L₀ + ... + ∂L₁₂₇) / 128

更新: w -= lr × grad


结论:
═══════════════════════════════════════════════════════════
方法 1: grad = (∂L₀ + ... + ∂L₁₂₇) / 128
方法 2: grad = (∂L₀ + ... + ∂L₁₂₇) / 128
方法 3: grad = (∂L₀ + ... + ∂L₁₂₇) / 128

完全相同！✓
```

### 4.2 本质是相同的分解策略

```
┌──────────────────────────────────────────────────────┐
│              大 batch 梯度的两种分解方式              │
├──────────────────────────────────────────────────────┤
│                                                       │
│ 核心问题:                                            │
│   如何高效计算 grad = Σ(∂Lᵢ/∂w) / N                │
│                                                       │
│ 策略 1: 梯度累积 (时间分解)                         │
│   将 N 个样本分成 K 组                               │
│   串行计算每组的梯度                                 │
│   累加所有梯度                                       │
│                                                       │
│ 策略 2: 数据并行 (空间分解)                         │
│   将 N 个样本分成 K 组                               │
│   并行计算每组的梯度 (在 K 个 GPU 上)               │
│   All-Reduce 求平均                                  │
│                                                       │
│ 数学上:                                              │
│   两者都是: Σ(∂Lᵢ/∂w) / N                          │
│                                                       │
│ 区别:                                                │
│   - 梯度累积: 在时间上分 K 步                       │
│   - 数据并行: 在空间上用 K 个 GPU                   │
│                                                       │
└──────────────────────────────────────────────────────┘
```

---

## 5. 时间和空间权衡

### 5.1 对比表

```
┌────────────────┬──────────────┬──────────────┬──────────────┐
│   方法         │   显存需求   │   时间       │   GPU 利用   │
├────────────────┼──────────────┼──────────────┼──────────────┤
│ 直接训练       │              │              │              │
│ (batch=128)    │   很高       │   T          │   1 GPU      │
│                │   ~4 GB      │              │   100%       │
├────────────────┼──────────────┼──────────────┼──────────────┤
│ 梯度累积       │              │              │              │
│ (4步×32)       │   低         │   4T         │   1 GPU      │
│                │   ~1 GB      │   (串行)     │   100%       │
├────────────────┼──────────────┼──────────────┼──────────────┤
│ 数据并行       │              │              │              │
│ (4 GPU×32)     │   低         │   T          │   4 GPU      │
│                │   ~1 GB/GPU  │   (并行)     │   各 100%    │
└────────────────┴──────────────┴──────────────┴──────────────┘
```

### 5.2 详细分析

```python
# ========== 场景: ResNet-50, 总 batch 128 张图 ==========

# 方法 1: 直接训练
# ─────────────────────────────────────────────────────
batch_size = 128
显存占用:
  - 模型参数: 100 MB
  - 梯度: 100 MB
  - 优化器: 100 MB
  - 激活值: 128 张 × 20 MB/张 = 2560 MB  ← 主要占用!
  - 总计: ~2.9 GB

时间: 50 ms
GPU 数量: 1


# 方法 2: 梯度累积
# ─────────────────────────────────────────────────────
batch_size = 32  # 小 batch
accumulation_steps = 4

显存占用:
  - 模型参数: 100 MB
  - 梯度: 100 MB (累积，不清零)
  - 优化器: 100 MB
  - 激活值: 32 张 × 20 MB/张 = 640 MB  ← 减少 4 倍!
  - 总计: ~940 MB

时间: 50 ms × 4 = 200 ms  ← 慢 4 倍
GPU 数量: 1

优点: 省显存 (可以在显存小的卡上训练大 batch)
缺点: 慢 4 倍


# 方法 3: 数据并行
# ─────────────────────────────────────────────────────
batch_size = 32  # 每个 GPU
num_gpus = 4

显存占用 (每个 GPU):
  - 模型参数: 100 MB
  - 梯度: 100 MB
  - 优化器: 100 MB
  - 激活值: 32 张 × 20 MB/张 = 640 MB
  - 总计: ~940 MB/GPU

时间: 50 ms  ← 和直接训练一样快!
GPU 数量: 4

优点: 省显存 + 快速
缺点: 需要多张 GPU
```

---

## 6. 混合使用

### 6.1 梯度累积 + 数据并行

```python
"""
最强组合: 梯度累积 + 数据并行

场景: 训练超大 batch 的模型 (如 GPT-3)
  - 总 effective batch size: 2048
  - 单卡显存有限: 只能处理 batch_size=8
  - 有 4 个 GPU
  
解决方案:
  - 数据并行: 4 GPU
  - 每 GPU batch: 8
  - 梯度累积: 64 步
  - Effective batch: 4 × 8 × 64 = 2048 ✓
"""

# ========== 完整代码 ==========

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# 配置
world_size = 4              # 4 个 GPU
batch_per_gpu = 8           # 每 GPU 的 mini-batch
accumulation_steps = 64     # 累积 64 步
effective_batch = world_size * batch_per_gpu * accumulation_steps
# = 4 × 8 × 64 = 2048

model = DDP(model)

for epoch in range(epochs):
    for step, batch in enumerate(dataloader):
        # batch_per_gpu = 8 张图
        
        # 前向
        outputs = model(batch)
        loss = criterion(outputs, labels)
        
        # 归一化 (除以累积步数)
        loss = loss / accumulation_steps
        
        # 反向 (梯度累加)
        loss.backward()
        
        # 每 64 步更新一次
        if (step + 1) % accumulation_steps == 0:
            # 此时:
            # 1. 每个 GPU 累积了 64 个 mini-batch 的梯度
            #    grad_gpu0 = sum of 64 mini-batches (8 images each)
            # 
            # 2. DDP 自动 All-Reduce 同步梯度
            #    grad_avg = mean(grad_gpu0, ..., grad_gpu3)
            #
            # 3. 最终梯度来自:
            #    4 GPU × 64 steps × 8 images = 2048 images!
            
            optimizer.step()
            optimizer.zero_grad()
```

### 6.2 数学分析

```
混合方法的梯度计算:

GPU 0 累积 64 步:
  grad₀ = Σ(step=0 to 63) [(∂L₀+...+∂L₇)/8] / 64
        = (512 张图的梯度) / 512

GPU 1 累积 64 步:
  grad₁ = (512 张图的梯度) / 512

GPU 2, GPU 3 同理...

All-Reduce:
  grad_final = (grad₀ + grad₁ + grad₂ + grad₃) / 4
             = (2048 张图的梯度) / 2048

这就是 effective batch size = 2048 的梯度！
```

### 6.3 可视化对比

```
┌──────────────────────────────────────────────────────────┐
│          梯度累积 + 数据并行 示意图                       │
│                                                           │
│  时间维度 (累积 64 步)                                    │
│  ↓                                                        │
│  Step 1 ─→ GPU 0: batch 8   GPU 1: batch 8  ... GPU 3   │
│  Step 2 ─→ GPU 0: batch 8   GPU 1: batch 8  ... GPU 3   │
│  Step 3 ─→ GPU 0: batch 8   GPU 1: batch 8  ... GPU 3   │
│  ...                                                      │
│  Step 64 ─→ GPU 0: batch 8  GPU 1: batch 8  ... GPU 3   │
│            ↓                 ↓                  ↓         │
│         grad_0_accum    grad_1_accum      grad_3_accum   │
│            │                 │                  │         │
│            └─────────────────┴──────────┬───────┘         │
│                                         ↓                 │
│                                  All-Reduce               │
│                                         ↓                 │
│                                    grad_final             │
│                                         ↓                 │
│                                  Update params            │
│                                                           │
│  Total images: 4 GPU × 64 steps × 8 batch = 2048        │
└──────────────────────────────────────────────────────────┘
```

---

## 7. 实际应用场景

### 7.1 选择决策树

```
┌─────────────────────────────────────────────────────────┐
│             如何选择训练策略？                           │
└─────────────────────────────────────────────────────────┘
                         │
                         ▼
            ┌────────────────────────┐
            │ 有多个 GPU 吗？         │
            └────────┬───────────────┘
                     │
         ┌───────────┴──────────────┐
         │ NO                       │ YES
         ▼                          ▼
    ┌─────────┐              ┌─────────────┐
    │ 单 GPU  │              │ 多 GPU      │
    └────┬────┘              └──────┬──────┘
         │                          │
         ▼                          ▼
    显存够吗？              batch 够大吗？
         │                          │
    ┌────┴─────┐              ┌─────┴─────┐
    │          │              │           │
   YES        NO             YES         NO
    │          │              │           │
    ▼          ▼              ▼           ▼
 ┌─────┐  ┌────────┐    ┌─────────┐  ┌─────────┐
 │直接 │  │梯度累积│    │数据并行 │  │  DDP +  │
 │训练 │  │        │    │         │  │梯度累积 │
 └─────┘  └────────┘    └─────────┘  └─────────┘
```

### 7.2 具体场景

```
场景 1: 小模型，单 GPU，显存充足
════════════════════════════════════════════════════════
例子: ResNet-50 在 16GB V100
推荐: 直接训练 (batch_size=256)
理由: 最简单，最快


场景 2: 大模型，单 GPU，显存不足
════════════════════════════════════════════════════════
例子: BERT-Large 在 11GB RTX 2080 Ti
推荐: 梯度累积
配置:
  - batch_size=4 (受限于显存)
  - accumulation_steps=32
  - effective_batch=128
理由: 省显存，可以训练


场景 3: 中等模型，4 GPU，显存充足
════════════════════════════════════════════════════════
例子: ResNet-50 在 4×16GB T4
推荐: 数据并行
配置:
  - 4 GPU DDP
  - batch_size=64/GPU
  - total_batch=256
理由: 快速，充分利用 GPU


场景 4: 超大模型，多 GPU，大 batch 训练
════════════════════════════════════════════════════════
例子: GPT-3 风格模型在 8×A100
推荐: DDP + 梯度累积
配置:
  - 8 GPU DDP
  - batch_size=2/GPU (模型太大)
  - accumulation_steps=64
  - effective_batch=1024
理由: 唯一可行方案
```

### 7.3 实际案例

```python
# ========== 案例 1: BERT 预训练 ==========
# 场景: 单卡 RTX 3090 (24GB)，BERT-Large (340M 参数)

# 问题: 直接 batch_size=32 会 OOM
# 解决: 梯度累积

model = BertForPreTraining.from_pretrained('bert-large')
batch_size = 8  # 小 batch，不 OOM
accumulation_steps = 4
effective_batch = 32

for step, batch in enumerate(dataloader):
    outputs = model(**batch)
    loss = outputs.loss / accumulation_steps
    loss.backward()
    
    if (step + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# 结果: 可以在单卡训练 BERT-Large with batch=32


# ========== 案例 2: GPT-2 分布式训练 ==========
# 场景: 8×A100 (80GB each)，GPT-2 (1.5B 参数)

# 问题: 
# - 模型很大，单卡只能 batch_size=4
# - 需要大 batch (512-1024) 稳定训练

# 解决: DDP + 梯度累积

model = DDP(GPT2LMHeadModel(...))
batch_per_gpu = 4
accumulation_steps = 16
effective_batch = 8 × 4 × 16 = 512

# 训练循环同上...

# 结果: 
# - 每 GPU 4 张图 (不 OOM)
# - 累积 16 步
# - 8 GPU 并行
# - Effective batch = 512 ✓


# ========== 案例 3: 你的 T4 环境 ==========
# 场景: 4×T4 (16GB each)，ResNet-50

# 推荐配置:
model = DDP(resnet50)
batch_per_gpu = 64  # T4 显存够用
# 不需要梯度累积
effective_batch = 4 × 64 = 256

# 为什么不用梯度累积？
# 因为 T4 显存足够处理 batch=64
# 直接数据并行就很高效


# 如果想训练更大的 batch:
batch_per_gpu = 32
accumulation_steps = 4
effective_batch = 4 × 32 × 4 = 512

# 权衡:
# - 更大的 batch (512 vs 256)
# - 但慢 4 倍 (因为累积)
```

---

## 8. 总结

### 8.1 核心要点

```
┌──────────────────────────────────────────────────────┐
│         梯度累积 vs 数据并行                          │
├──────────────────────────────────────────────────────┤
│                                                       │
│ 本质: 完全相同                                       │
│   都基于梯度的可加性                                 │
│   都是将大 batch 分解后累加                          │
│                                                       │
│ 区别: 执行方式                                       │
│   梯度累积: 时间上串行 (1 GPU, K 步)                │
│   数据并行: 空间上并行 (K GPU, 1 步)                │
│                                                       │
│ 数学: 完全等价                                       │
│   grad = Σ(∂Lᵢ/∂w) / N                             │
│                                                       │
│ 应用:                                                │
│   梯度累积: 单 GPU 显存不足                          │
│   数据并行: 多 GPU 加速训练                          │
│   混合使用: 超大 batch + 大模型                      │
│                                                       │
└──────────────────────────────────────────────────────┘
```

### 8.2 选择指南

```
┌────────────┬──────────┬──────────┬──────────┬──────────┐
│   场景     │ GPU 数量 │ 显存需求 │   推荐   │   速度   │
├────────────┼──────────┼──────────┼──────────┼──────────┤
│ 小模型     │    1     │   充足   │ 直接训练 │  最快    │
│ 大模型     │    1     │   不足   │ 梯度累积 │  慢 K 倍 │
│ 小模型     │   多     │   充足   │ 数据并行 │  最快    │
│ 大模型     │   多     │   不足   │ DDP+累积 │  中等    │
└────────────┴──────────┴──────────┴──────────┴──────────┘
```

### 8.3 你的洞察

```
你的问题抓住了本质! 

梯度累积和数据并行确实是同一数学原理:

  梯度的可加性
       ↓
  ∂(L₁+L₂)/∂w = ∂L₁/∂w + ∂L₂/∂w
       ↓
  可以分解求和
       ↓
  ┌─────────┴──────────┐
  │                    │
时间分解           空间分解
(梯度累积)        (数据并行)
  │                    │
串行执行           并行执行
慢但省显存         快但需多卡

这就是分布式训练的精髓！
```

---

## 9. 进阶话题

### 9.1 为什么大 batch 训练很重要？

```
大 batch 的优势:

1. 训练稳定性 ↑
   - 梯度噪声 ↓
   - 收敛更平滑

2. 泛化性能 (复杂)
   - 小 batch: 泛化好，但不稳定
   - 大 batch: 稳定，但可能过拟合
   - 需要调整学习率

3. 吞吐量 ↑
   - GPU 利用率更高
   - 减少通信频率

大模型训练 (如 GPT-3) 通常需要:
  batch_size = 512 ~ 2048
```

### 9.2 学习率缩放

```python
# 重要! 大 batch 需要调整学习率

# 线性缩放规则 (Linear Scaling Rule)
base_lr = 0.1       # batch=256 时的学习率
base_batch = 256

new_batch = 1024    # 使用梯度累积/DDP 后的大 batch
new_lr = base_lr * (new_batch / base_batch)
       = 0.1 * (1024 / 256)
       = 0.4

# 原因:
# batch 增大 → 梯度更稳定 → 可以用更大的步长
```

---

恭喜你发现了这个深刻的联系！这正是理解分布式训练的关键。🎯